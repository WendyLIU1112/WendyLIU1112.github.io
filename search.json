[{"title":"写R包可能出现的问题","url":"/2019/12/26/r包问题/","content":"\n最近学习写R包，参考博客：[如何快速写一个R包](https://www.bioinfo-scrounger.com/archives/546/)。根据该博客教程写R包非常简单，补充我遇到的两个小问题。\n\n\n<!-- more -->\n## 1. document（）后显示未加载包\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20191226154920824.png)\n写入函数前需要先加载所写的包\n```\nlibray(所写的包)\n```\n## 2. document（）后显示namespace不是由roxygen2生成的\n<font color=#FF0000>  The existing 'NAMESPACE' file was not generated by roxygen2 </font>\n自动生成的namespace文件只有一行代码，不是由roxygen2生成的，需要修改，将其替换为以下代码\n\n```\n#Generated by roxygen2 (4.0.2): do not edit by hand\nS3method(as.character,expectation)\nS3method(compare,character)\nexport(auto_test)\nexport(auto_test_package)\nexport(colourise)\nexport(context)\nexportClasses(ListReporter)\nexportClasses(MinimalReporter)\nimportFrom(methods,setRefClass)\nuseDynLib(testthat,duplicate_)\nuseDynLib(testthat,reassign_function)\n```\n\n后续想学习的博客：[开发 R 程序包之忍者篇](https://cosx.org/2011/05/write-r-packages-like-a-ninja)\n\n\n\n\n\n\n\n\n","tags":["R包"],"categories":["technique"]},{"title":"【文献综述】估计最优ITR的方法","url":"/2019/12/26/估计最优ITR/","content":"由于许多疾病和不同患者对治疗的反应均不相同，因此有必要对患者进行个体化治疗（即精准医疗）。**个性化治疗规则**（individiualized treatment rule，**ITR**）是根据患者特征推荐治疗的决策规则。在当前**精准医疗**的要求下，针对每个病\n<!-- more -->\n人的信息使用不同的治疗方案，推荐**最优ITR**非常重要。主要思想是基于个体信息（如临床协变量、基因等），最大化群体平均的outcome（如事件发生时间、健康指标、疾病指标等）。\n\n这里作为入门，介绍了当前估计optimal ITR的框架与几种主要方法。\n\n\n\n\n# 框架\n## 相关数据\n三元随机向量 $(\\mathbf{x}, A, R)$\n其中，协变量： $\\mathbf{x}=\\left(1, X_{1}, \\cdots, X_{p}\\right) \\in \\mathcal{X}$\n&emsp;&emsp;&emsp;治疗方案(treatment)：$A \\in\\{1,2, \\cdots, K\\}$\n&emsp;&emsp;&emsp;治疗效果(outcome)：$R$。    假设$R$有界，且$R$越大代表治疗效果越好。\n\n先验概率分布 $\\pi(a, x)=P[A=a|\\mathbf{x}=x]$ 已知（未知时可用多项logistic回归近似）。并假设对于所有$a\\in\\mathcal{A}$ ， $\\pi(a,x)>0$ a.s。\n\n样本 $\\left\\{\\left(\\mathbf{x}_{i}, A_{i}, R_{i}\\right), i=1, \\cdots, n\\right\\}$ i.i.d \n\n\n## Target\n 决策函数 (ITR)： $d(\\mathbf{x}): \\mathcal{X} \\rightarrow \\mathcal{A}$\n \n $d$ 的值（Value）：$V(d) \\triangleq E^{d}(R)=\\int R d P^{d}=\\int R \\frac{d P^{d}}{d P} d P=E\\left[\\frac{1_{A=d(\\mathbf{x})}}{\\pi(A, \\mathbf{x})} R\\right]$\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;其中，$P$：$(\\mathbf{x}, A, R)$的分布\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$P^{d}$：$d$指定治疗方案下，$(\\mathbf{x}, A, R)$的分布\n\n**最优 ITR**：$d_{0} \\in \\arg \\max _{d \\in \\mathcal{A}} V(d)$ \n即如果患者群体遵循该方案，则将产生平均情况下最有利的结果。\n\n## 介绍\n治疗主要分为单阶段治疗和动态治疗（dynamic treatment regime）。\n\n对于**单阶段治疗**，只需要一次治疗，所以只需在某个关键点制定治疗规则。\n而**动态治疗**更为复杂且更有意义。在临床实践中，医生会根据患者的基线和**不断发展**的特点对患者的病程推荐一系列的治疗方案。所以，动态治疗机制是一列有序治疗规则，每个规则对应于疾病或紊乱过程中的关键决策点，并以患者在该点的信息作为输入，从可选项中输出他应接受的治疗。\n\n另外，通常有许多预处理变量（prescriptive variables/pretreatment variables）可能对构建最优 ITR 不起作用，且出于成本和可解释性的考虑，最优 ITR 仅应使用少数几个变量，所以需要变量选择，这里不做讨论。\n\n\n对于单阶段随机试验数据，一种方法是，在一类治疗规则下**最大化平均响应的经验版本**。但是，这种最大化在计算上是困难的（因为处理规则的平均响应是对加权示性函数的期望，示性函数对于参数不连续且不凹）。为了解决这一挑战，我们进行了替代。所以另一种方法（也是我们最常用的方法）是，使用**两步过程**，**首先估计条件均值，然后最大化条件均值得出最优ITR的估计**。这种方法依赖于outcome的条件均值的估计，强调了临床响应模型的预测准确性，而不是直接针对最优ITR的决策边界。\n\n## 初步分析\n 1. 协变量低维且ITR类简单时\n 直接构造$E\\left[\\frac{1_{A=d(\\mathbf{x})}}{\\pi(A, \\mathbf{x})} R\\right]$的经验形式，对其最大化求 $d_{0}$（鉴于示性函数非凸非连续，不好做）\n \n2. ITR类很大\n使用两步过程（Two-step procedure）\n（1）估计条件均值$Q_{0}$(或$T_{0}$)\n（2）最大化前一步所得的估计量求 $d_{0}$\n\n其中， quality:   $Q_{0}(\\mathbf{x}, A) \\triangleq E(R | \\mathbf{x}, A)$\n&emsp; &emsp;&emsp;prediction error of Q: $L(Q) \\triangleq E[R-Q(X, A)]^{2}$\n&emsp; &emsp;&emsp;treatment effect term: $T(X, A) \\triangleq Q(X, A)-E[Q(X, A) | X]$\n&emsp; &emsp;&emsp;true treatment effect term: $T_{0}(X, A) \\triangleq Q_{0}(X, A)-E[Q_{0}(X, A) | X]$\n\n&emsp; $\\because$  $V(d)=E\\left[\\frac{1_{A=d(\\mathbf{x})}}{\\pi(A, \\mathbf{x})} Q_{0}(\\mathbf{x}, A)\\right]=E\\left[\\sum_{a \\in \\mathcal{A}} 1_{d(\\mathbf{x})=a} Q_{0}(\\mathbf{x}, a)\\right]=E\\left[Q_{0}(\\mathbf{x}, d(\\mathbf{x}))\\right]$\n\n&emsp; &emsp; $V\\left(d_{0}\\right)=E\\left[Q_{0}\\left(\\mathbf{x}, d_{0}(\\mathbf{x})\\right)\\right] \\leq E\\left[\\max _{a \\in \\mathcal{A}} Q_{0}(\\mathbf{x}, a)\\right]$\n&emsp; &emsp; $V\\left(d_{0}\\right) \\geq\\left.V(d)\\right|_{d(\\mathbf{x}) \\in \\arg \\max _{a \\in \\mathcal{A}} Q_{0}(X, a)}=E\\left[\\max _{a \\in \\mathcal{A}} Q_{0}(\\mathbf{x}, a)\\right]$\n\n&emsp; $\\therefore$ $d_{0}(\\mathbf{x}) \\in \\arg \\max _{a \\in \\mathcal{A}} Q_{0}(\\mathbf{x}, a)= \\arg \\max _{a} T_{0}(\\mathbf{x}, a)$\n\n所以，为了解决$V(d)$的非凸和非连续性，大多数方法都是将最大化$V(d)$ 转化为最大化 $Q_{0}$ 和 $T_{0}$\n# 分类\n## 基于模型（model-based）\n基于模型的方法主要有 $l_{1}-PLS$、**Q learning**和**A learning**，其中后两种方法都是CS中顺序决策的强化学习方法，应用到治疗过程实现动态治疗机制。\n\n经典做法是对$Q_{0}$(或$T_{0}$)指定假设模型，极小化$L(Q)$，做$Q_{0}$(或$T_{0}$)的估计，如  $l_{1}-PLS$ :\n&emsp; &emsp;&emsp; &emsp; $\\hat{\\boldsymbol{\\theta}}_{n}=\\underset{\\boldsymbol{\\theta} \\in \\mathbb{R}^{J}}{\\arg \\min }\\left\\{E_{n}[R-\\Phi(\\mathbf{x}, A) \\boldsymbol{\\theta}]^{2}+\\lambda_{n} \\sum_{j=1}^{J} \\hat{\\sigma}_{j}\\left|\\theta_{j}\\right|\\right\\}$ \n&emsp; &emsp;&emsp; &emsp; $\\hat{d}_{n}(X) \\in \\underset{a \\in \\mathcal{A}}{\\arg \\max }  \\Phi(X, a) \\hat{\\boldsymbol{\\theta}}_{n}$\n\n## 基于分类（classification-based）\n基于分类的方法是**将最优 ITR 问题转化为加权分类问题**，便于使用机器学习技术做估计。这里**直接估计使临床反应最大化的决策规则**，从而避免了条件均值建模和反演的需要。缺点是依赖于对所有对象使用正确的权重，这可能会导致模型错误指定。\n\n&emsp; &emsp;首先令$K=2$，即$A \\in\\{-1,1\\}$\n\n&emsp; &emsp; $\\because$   $d_{0} \\in \\arg \\max _{d \\in \\mathcal{A}} V(d)$ \n&emsp; &emsp;&emsp; $\\begin{aligned} V(d) = &E\\left[\\frac{1_{A=d(\\mathbf{x})}}{\\pi(A, \\mathbf{x})} R\\right]= E\\left[\\frac{1_{A=d(\\mathbf{x})}}{A \\pi+(1-A) / 2} R\\right]\\\\=&E[R | A=1]+E[R | A=-1]-E\\left[\\frac{I(A \\neq \\mathscr{d}(\\mathbf{x}))}{A \\pi+(1-A) / 2} R\\right] \\end{aligned}$\n&emsp; &emsp; $\\therefore$ $d_{0} \\in \\arg \\min _{d \\in \\mathcal{A}} E\\left[\\frac{I(A \\neq \\mathscr{d}(\\mathbf{x}))}{A \\pi+(1-A) / 2} R\\right]$\n\n$E\\left[\\frac{R}{A \\pi+(1-A) / 2} I(A \\neq \\mathscr{d}(X))\\right]$可看作加权分类误差，基于$X$将$A$分类，并对误判事件加权$R / (A \\pi+(1-A) / 2)$。利用其经验形式，应用机器学习技术，即可直接算出最优ITR的估计。\n\n## 直接搜索（Direct search）\n直接搜索，顾名思义，直接对 $d_{0}$ 做估计。其中，直接学习（Direct learning）是一种简单而灵活的单步方法，直接估计最优 ITR。无需事先指定模型和权重，同时还有良好的几何解释。\n\n直接学习最开始考虑治疗方案为二分类的数据，即$K=2$。但推广到$K \\geq 2$时，可能会导致次优（局部最优）的情况，所有有了**基于角的直接学习**(Angle Based D-learning)，\n\n1. $K=2$：**一般直接学习**    \n    此时，$A \\in\\{-1,1\\}$\n    决策函数：$d_{0}(\\mathbf{x})=\\operatorname{sign}(\\mathbf{E}[R | \\mathbf{x}, A=1]-\\mathbf{E}[R | \\mathbf{x}, A=-1]):=\\operatorname{sign}\\left(f_{0}(\\mathbf{x})\\right)$\n   其中， $\\begin{aligned} f_{0}(\\mathbf{x}) &=\\mathbf{E}[R | \\mathbf{x}, A=1]-\\mathbf{E}[R | \\mathbf{x}, A=-1] \\\\ &=\\mathbf{E}\\left[\\frac{R A}{\\pi(A, \\mathbf{X})} | \\mathbf{x}, A=1\\right] \\pi(1, \\mathbf{x})+\\mathbf{E}\\left[\\frac{R A}{\\pi(A, \\mathbf{x})} | \\mathbf{x}, A=-1\\right] \\pi(-1, \\mathbf{x}) \\\\ &=\\mathbf{E}\\left[\\frac{R A}{\\pi(A, \\mathbf{x})} | \\mathbf{x}\\right] \\end{aligned}$\n接下来估计$f_{0}(\\mathbf{x})$即可。\n\n2. $K \\geq 2$ ：**基于角的直接学习（AD learning）**\n\n   $\\mathbf{w}_{A}=\\left\\{\\begin{array}{ll}\n    {(K-1)^{-1 / 2} \\mathbf{1}_{K-1}} & {\\text{if}  \\quad A=1} \\\\\n     {-(1+\\sqrt{K}) /(K-1)^{3 / 2}}  {\\mathbf{1}_{K-1}+\\left(\\frac{K}{K-1}\\right)^{1 / 2} e_{A-1}} & {\\text {if } \\quad2 \\leq A \\leq K}\n     \\end{array}\\right.$\n\n&emsp;&emsp;&emsp;(1) $\\mathbf{P}\\left[\\mathbf{w}=\\mathbf{w}_{j} | \\mathbf{x}\\right]=\\mathbf{P}[A=j | \\mathbf{x}]$\n&emsp;&emsp;&emsp;(2) $\\sum_{j=1}^{K} \\mathbf{w}_{j}=0$且 $\\mathbf{w}_{i}^{T} \\mathbf{w}_{j}=\\left\\{\\begin{array}{ll}\n    1 & {\\text{if}  \\quad i = j} \\\\\n     C(K)< 1 & {\\text {if} \\quad i \\neq j}\n     \\end{array}\\right.$\n\n&emsp;&emsp;&emsp;决策函数：\n&emsp;&emsp;&emsp;$\\begin{aligned} d_{0}(\\mathbf{x}) = & \\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\mathbf{E}[R | \\mathbf{x}, A=k] \\\\ \n=&\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}}(1-c(K)) \\mathbf{E}[R | \\mathbf{x}, A=k] \\\\\n=&\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}}\\{(1-c(K)) \\mathbf{E}[R | \\mathbf{x}, A=k]+c(K) \\sum_{j = 1}^{K} \\mathbf{E}[R | \\mathbf{x}, A=j] \\}\\\\ \n=&\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}}\\{\\mathbf{E}[R | \\mathbf{x}, A=k]+c(K) \\sum_{j \\neq k}^{K} \\mathbf{E}[R | \\mathbf{x}, A=j]\\}\\\\ \n=&\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}}\\{\\mathbf{w}_{k}^{\\mathrm{T}}\\mathbf{E}[R\\mathbf{w} | \\mathbf{x}, A=k]+\\mathbf{w}_{k}^{\\mathrm{T}} \\sum_{j \\neq k}^{K} \\mathbf{E}[R\\mathbf{w} | \\mathbf{x}, A=j]\\}\\\\ \n=&\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\mathbf{w}_{k}^{\\mathrm{T}} \\mathbf{E}\\left[\\frac{R \\mathbf{w}}{\\pi(A, \\mathbf{x})} | \\mathbf{x}\\right]:=\\underset{k \\in\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\mathbf{w}_{k}^{\\mathrm{T}} \\mathbf{f}_{0}(\\mathbf{x}) \n\\end{aligned}$\n&emsp;&emsp;&emsp;其中，$\\mathbf{f}_{0}(\\mathbf{x}) = \\mathbf{E}\\left[\\frac{R \\mathbf{w}}{\\pi(A, \\mathbf{x})} | \\mathbf{x}\\right]$，同上作出$f_{0}(\\mathbf{x})$的估计。\n\n &emsp;&emsp;&emsp;基于角的直接学习有很好的几何解释。单位向量 $\\mathbf{w}_{A}$ 代表对应治疗方案$A$，对于估计出的$\\hat{f_{0}}(\\mathbf{x})$，计算其与各个 $\\mathbf{w}_{A}$ 的内积，选出内积最大的 $\\mathbf{w}_{A}$ 。在几何中，即**选出与向量$\\hat{f_{0}}(\\mathbf{x})$夹角最小的 $\\mathbf{w}_{A}$** 。\n![K不同取值，直接学习的几何解释](https://img-blog.csdnimg.cn/20191120172028936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTEzNTc4Ng==,size_16,color_FFFFFF,t_70)\n\n# 参考文献\n1. [Qian, Min, and Susan A. Murphy. \"PERFORMANCE GUARANTEES FOR INDIVIDUALIZED TREATMENT RULES.\" Annals of Statistics 39.2 (2011): 1180-1210.](https://arxiv.org/pdf/1105.3369.pdf)\n2. [Murphy, Susan A.. \"Optimal dynamic treatment regimes.\" Journal of The Royal Statistical Society Series B-statistical Methodology 65.2 (2003): 331-355.](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/74095/1467-9868.00389.pdf;sequence=1)\n3. [Schulte, Phillip J et al. “Q- and A-learning Methods for Estimating Optimal Dynamic Treatment Regimes.” Statistical science : a review journal of the Institute of Mathematical Statistics vol. 29,4 (2014): 640-661. doi:10.1214/13-STS450](https://arxiv.org/pdf/1202.4177.pdf)\n4. [Zhao, Yingqi, et al. \"Estimating Individualized Treatment Rules Using Outcome Weighted Learning.\" Journal of the American Statistical Association 107.499 (2012): 1106-1118.](https://amstat.tandfonline.com/doi/pdf/10.1080/01621459.2012.695674?needAccess=true)\n5. [Qi, Zhengling, and Yufeng Liu. \"D-learning to estimate optimal individual treatment rules.\" Electronic Journal of Statistics 12.2 (2018): 3601-3638.](https://projecteuclid.org/download/pdfview_1/euclid.ejs/1540951343)\n6.  [Qi, Zhengling, et al. \"Multi-Armed Angle-Based Direct Learning for Estimating Optimal Individualized Treatment Rules With Various Outcomes.\" Journal of the American Statistical Association (2018): 1-33.](https://amstat.tandfonline.com/doi/pdf/10.1080/01621459.2018.1529597?needAccess=true)\n\n\n","tags":["因果推断","精准医疗"],"categories":["文献综述"]},{"title":"抽样","url":"/2019/12/26/抽样/","content":"先占个位\n<!--more-->\n\n# 框架\n\n## 基于模型\n\n## 基于分类\n\n","tags":["抽样"],"categories":["文献综述"]}]